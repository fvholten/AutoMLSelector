{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to evaluate the ML-Models\n",
    "\n",
    "Setup (load automl-models.pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "with open(r\"../app/_meta.csv\") as csv_file:\n",
    "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "  list_qualities = list()\n",
    "  for line in csv_reader:\n",
    "    list_qualities.append(line[0])\n",
    "  list_qualities = list_qualities[1:]\n",
    "\n",
    "\n",
    "with open(r\"../app/ml-models.pickle\", \"rb\") as input_file:\n",
    "  ml_models = pickle.load(input_file)\n",
    "\n",
    "\n",
    "dataset_loc = 'training-data/'\n",
    "\n",
    "datasets_names = [f for f in listdir(dataset_loc) if isfile(join(dataset_loc, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_scores = 0\n",
    "for key, value in ml_models.items():\n",
    "  print(key, ':', value.get('model').best_loss)\n",
    "  sum_scores += value.get('model').best_loss\n",
    "print()\n",
    "print('Average Loss:', \"{:.2f}\".format(100*sum_scores/len(ml_models), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a random dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml as oml\n",
    "\n",
    "dataset = oml.datasets.get_dataset(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = dict()\n",
    "\n",
    "for quality in list_qualities:\n",
    "  t[quality] = [dataset.qualities.get(quality)]\n",
    "test_df = pd.DataFrame(t)\n",
    "\n",
    "for key, value in ml_models.items():\n",
    "  aml = value.get('model')\n",
    "  print(key, ':', aml.predict_proba(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 3) # (w, h)\n",
    "\n",
    "def make_feature_importance_diagram(feature_names_in, feature_importances, name):\n",
    "  sorted_idx = feature_importances.argsort()[-5:]\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.barh(\n",
    "    feature_names_in[sorted_idx],\n",
    "    feature_importances[sorted_idx],\n",
    "    height=0.5,\n",
    "\n",
    "  )\n",
    "  plt.xlabel(\"Feature importance\")\n",
    "  plt.ylabel(\"Features\")\n",
    "  plt.savefig('graphs/feature-importances/{}-features.png'.format(name), dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "def make_feature_importance_diagram_lgb(model, name):\n",
    "  importance_types = ['gain', 'split']\n",
    "  for importance_type in importance_types:\n",
    "    lgb.plot_importance(model.estimator, max_num_features=5, importance_type=importance_type, title='', height=0.5, grid=False, figsize=(4,3))\n",
    "    plt.savefig('graphs/feature-importances/{}-features ({}).png'.format(name, importance_type), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TPOT\n",
    "feature_names_in = ml_models['TPOT'].get('model').model.estimator.feature_names_in_\n",
    "make_feature_importance_diagram(feature_names_in, ml_models['TPOT'].get('model').model.estimator.feature_importances_, 'TPOT')\n",
    "\n",
    "#Hyperopt-Sklearn\n",
    "make_feature_importance_diagram(feature_names_in, ml_models['Hyperopt-Sklearn'].get('model').model.estimator.feature_importances_, 'Hyperopt-Sklearn')\n",
    "\n",
    "#AutoGluon\n",
    "make_feature_importance_diagram(feature_names_in, ml_models['AutoGluon'].get('model').model.estimator.feature_importances_, 'AutoGluon')\n",
    "\n",
    "#ATM\n",
    "make_feature_importance_diagram(feature_names_in, ml_models['ATM'].get('model').model.estimator.feature_importances_, 'ATM')\n",
    "\n",
    "#TransmogrifAI\n",
    "make_feature_importance_diagram(feature_names_in, ml_models['TransmogrifAI'].get('model').model.estimator.feature_importances_, 'TransmogrifAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-sklearn\n",
    "make_feature_importance_diagram_lgb(ml_models['auto-sklearn'].get('model').model, 'auto-sklearn')\n",
    "\n",
    "#H20 AutoML\n",
    "make_feature_importance_diagram_lgb(ml_models['H2O AutoML'].get('model').model, 'H2O AutoML')\n",
    "\n",
    "# FLAML\n",
    "make_feature_importance_diagram_lgb(ml_models['FLAML'].get('model').model, 'FLAML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "from flaml.data import get_output_from_log\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5.5, 4.5) # (w, h)\n",
    "\n",
    "logs = [\n",
    "'ATM-data.csv2022-09-09 19:02:59.228622.log',\n",
    "'auto-sklearn-data.csv2022-09-09 18:52:58.964856.log',\n",
    "'AutoGluon-data.csv2022-09-09 19:00:59.162002.log',\n",
    "'FLAML-data.csv2022-09-09 18:58:59.078054.log',\n",
    "'H2O AutoML-data.csv2022-09-09 18:56:59.124596.log',\n",
    "'Hyperopt-Sklearn-data.csv2022-09-09 18:54:59.051468.log',\n",
    "'TPOT-data.csv2022-09-09 18:50:58.856981.log',\n",
    "'TransmogrifAI-data.csv2022-09-09 19:04:59.343437.log']\n",
    "\n",
    "for log in logs:\n",
    "    time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename='logs/' + log, time_budget=60*2)\n",
    "    plt.scatter(time_history, 1 - np.array(valid_loss_history), label=log[0:-39])\n",
    "    plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('graphs/learning-curves.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
